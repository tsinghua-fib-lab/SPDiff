import os
import argparse
import torch
import dill
# import pdb
import numpy as np
import os.path as osp
import logging
import time
from torch import nn, optim, utils
import torch.nn as nn
from tensorboardX import SummaryWriter
from tqdm.auto import tqdm

# import pickle
# from environment.TrajectoryDS import TrajectoryDataset

from torch.utils.data import DataLoader
from dataset import EnvironmentDataset, collate, get_timesteps_data, restore
from models.autoencoder import AutoEncoder
# from models.trajectron import Trajectron
# from utils.model_registrar import ModelRegistrar
from utils import data_loader as LOADER
from utils.trajectron_hypers import get_traj_hypers
from utils.utils import mask_mse_func, post_process
from utils.visualization import plot_trajectory
import evaluation
import data.data as DATA
import data.dataset as DATASET
import torch.nn.functional as F
import utils.metrics as METRIC

class SPDiff():
    def __init__(self, config):
        self.config = config
        torch.backends.cudnn.benchmark = True # explanation: https://blog.csdn.net/leviopku/article/details/121661020
        self._build()

    def train(self):
        
        
        train_loaders = self.finetune_train_loaders
        val_list = self.finetune_valid_list
        optimizer = self.ft_optimizer
        scheduler = self.ft_scheduler
        total_epochs = self.config.total_epochs
        finetune_flag = True
        train_data_num = int(len(train_loaders)*self.config.train_data_ratio)
        train_loaders = train_loaders[:train_data_num]
        loss_epoch = {}
        for epoch in range(total_epochs+1):
            if epoch!=0:
                loss_epoch[epoch]=[]
                # self.train_dataset.augment = self.config.augment
                # if self.config.train_mode=='origin':
                #     mse_list = []
                #     # for node_type, data_loader in self.train_data_loader.items():
                #     pbar = tqdm(list(itertools.islice(self.dataloader_train,10)), ncols=80)
                #     node_type='pedestrian'
                #     loader_len = len(pbar)
                #     is_fst_loss = True
                #     loss_batch = 0
                #     batch_cnt = 0
                    
                #     self.optimizer.zero_grad()
                #     for cnt, batch in enumerate(pbar):
                        
                #         train_loss = self.model.get_loss(batch, node_type)
                        
                #         if (cnt+1)%self.config.batch_size !=0 and cnt != loader_len-1 :
                #             if is_fst_loss :
                #                 loss = train_loss
                #                 is_fst_loss = False
                #             else:
                #                 loss += train_loss

                #         else:
                #             batch_cnt += 1
                #             loss = train_loss # xiugai
                #             loss = loss/self.config.batch_size*10000
                #             is_fst_loss = True
                #             loss.backward()
                #             self.optimizer.step()
                #             self.optimizer.zero_grad()
                #             loss_batch += loss.item()
                #             loss_epoch[epoch].append(loss.item())
                            
                #             mse_list.append(loss.item())
                #         pbar.set_description(f"Epoch {epoch}, {batch_cnt}/{len(pbar)//self.config.batch_size+1} MSE: {loss_batch/(batch_cnt if batch_cnt else 1):.10f}")
                if self.config.train_mode=='multi':
                    mse_list = []
                    for i, train_loader in enumerate(train_loaders):
                        if self.config.finetune_trainmode=='singlestep' or not finetune_flag:
                            pbar = tqdm(train_loader, ncols=90)
                            # pbar = tqdm(list(itertools.islice(train_loaders,1)), ncols=90)
                            for batch_idx, batch_data in enumerate(pbar):
                                self.optimizer.zero_grad()
                                self.batch_idx = batch_idx
                                train_loss = self.model.get_loss(batch_data)
                                if self.config.diffnet!='SpatialTransformer_dest_force':
                                    train_loss.backward()
                                    optimizer.step()
                                    scheduler.step()
                                    print('last lr:',scheduler.get_last_lr())
                                loss_epoch[epoch].append(train_loss.item())
                                # mse_list.append(train_loss.item())
                                pbar.set_description(f"Epoch {epoch}, {batch_idx+1}/{len(pbar)},{i+1}/{len(self.train_loaders)} MSE: {np.mean(loss_epoch[epoch])}")
                        elif self.config.finetune_trainmode=='multistep':
                            self.optimizer.zero_grad()
                            # torch.autograd.set_detect_anomaly(True)
                            assert type(train_loader)==DATA.ChanneledTimeIndexedPedData
                            loss = self.model.test_multiple_rollouts_for_training_geo(train_loader)
                            # loss = self.model.test_multiple_rollouts_for_training(train_loader)

                            loss_epoch[epoch].append(loss.item())
                            loss.backward()
                            optimizer.step()
                            scheduler.step()
                            print('last lr:',scheduler.get_last_lr())
                            mse_mean = np.mean(loss_epoch[epoch])
                            print(f"Epoch {epoch} NO {i} MSE: {mse_mean} ")
                            del loss
                            # import gc
                            # gc.collect()
                            # torch.cuda.empty_cache()
                    
                self.log_writer.add_scalar('train_MSE', np.mean(loss_epoch[epoch]), epoch)
            
            # self.train_dataset.augment = False
            if (epoch) % self.config.eval_every == 0:
                self.model.eval()

                node_type = "PEDESTRIAN"
                eval_ade_batch_errors = []
                eval_fde_batch_errors = []
                if self.config.train_mode =='origin':
                    pbar = tqdm(self.dataloader_eval)
                    i = 0
                    for batch in pbar:
                        i+=1
                        traj_pred = self.model.generate2(batch, node_type, num_points=0,sample=20,bestof=True)
                        gt = batch[3].squeeze().permute(2,0,1)
                elif self.config.train_mode == 'multi':
                    if self.config.val_mode=='multistep':
                        mse_list = []
                        mae_list = []
                        ot_list = []
                        FDE_list = []
                        mmd_list = []
                        collision_list = []
                        dtw_list = []
                        ipd_list = []
                        ipd_mmd_list = []
                        for i, val_data in enumerate(val_list):
                            with torch.no_grad():
                                traj_pred, dest_force, ped_force = self.model.generate_multistep_geo(val_data, t_start=self.config.skip_frames)
                                # traj_pred, dest_force, ped_force = self.model.generate_multistep(val_data, t_start=self.config.skip_frames)
                                
                                ipd_mmd = METRIC.get_nearby_distance_mmd(traj_pred.position, traj_pred.velocity, 
                                                                val_data.labels[..., :2], val_data.labels[..., 2:4], 
                                                                val_data.mask_p_pred.long(), self.config.dist_threshold_ped, self.config.topk_ped*2, reduction='mean')
                                ipd_mmd_list.append(ipd_mmd)
                                p_pred = traj_pred.position
                                # p_pred_ = p_pred.clone()
                                # p_pred_[:-1, :, :] = p_pred_[1:, :, :].clone()
                                # p_pred = p_pred_
                                mask_p_pred = val_data.mask_p_pred.long()  # (*c) t, n
                                labels = val_data.labels[..., :2]
                                
                                torch.save(labels, self.logs_dir+f'/{epoch}_labels.pth')
                                torch.save(p_pred, self.logs_dir+f'/{epoch}_p_pred.pth')
                                torch.save(mask_p_pred, self.logs_dir+f'/{epoch}_mask_p_pred.pth')
                                torch.save(dest_force, self.logs_dir+f'/{epoch}_dest_force.pth')
                                torch.save(ped_force, self.logs_dir+f'/{epoch}_ped_force.pth')
                                
                                # plot_trajectory(p_pred, labels, name=time.strftime('%Y-%m-%d-%H-%M'))
                                collision = METRIC.collision_count(p_pred, 0.5, reduction='sum')
                                FDE = METRIC.fde_at_label_end(p_pred, labels, reduction='mean')
                                
                                p_pred = post_process(val_data, p_pred, traj_pred.mask_p, mask_p_pred)
                                dtw = METRIC.dtw_tensor(p_pred, labels, mask_p_pred, mask_p_pred,reduction='mean')
                                dtw_list.append(dtw)
                        
                                
                                func = lambda x: x*torch.exp(-x)
                                ipd = METRIC.inter_ped_dis(p_pred, labels, mask_p_pred,reduction='mean',applied_func=func)
                                ipd_list.append(ipd)
                                
                                loss = F.mse_loss(p_pred[mask_p_pred == 1], labels[mask_p_pred == 1], reduction='mean')*2
                                loss = loss.item()
                                mse_list.append(loss)
                                mae = METRIC.mae_with_time_mask(p_pred, labels, mask_p_pred, reduction='mean')
                                mae_list.append(mae)

                            ot = METRIC.ot_with_time_mask(p_pred, labels, mask_p_pred, reduction='mean', dvs=self.config.device)
                            mmd = METRIC.mmd_with_time_mask(p_pred, labels, mask_p_pred, reduction='mean')
                            
                            ot_list.append(ot)
                            mmd_list.append(mmd)
                            FDE_list.append(FDE)
                            collision_list.append(collision)
                            # ade,fde = evaluation.compute_batch_statistics2(traj_pred,gt,best_of=True)
                            # eval_ade_batch_errors.append(ade)
                            # eval_fde_batch_errors.append(fde)
                        # ade = np.mean(eval_ade_batch_errors)
                        # fde = np.mean(eval_fde_batch_errors) 
                        mse = np.mean(mse_list)
                        mae = np.mean(mae_list)
                        ot = np.mean(ot_list)
                        FDE = np.mean(FDE_list)
                        mmd = np.mean(mmd_list)
                        collision = np.mean(collision)
                        dtw = np.mean(dtw_list)
                        ipd = np.mean(ipd_list)
                        ipd_mmd = np.mean(ipd_mmd_list)
                        # if self.config.dataset == "eth":
                        #     ade = ade/0.6
                        #     fde = fde/0.6
                        # elif self.config.dataset == "sdd":
                        #     ade = ade * 50
                        #     fde = fde * 50
                        # print(f"Epoch {epoch} Best Of 20: ADE: {ade} FDE: {fde}")
                        print(f"Epoch {epoch} MSE: {mse} MAE: {mae}")
                        print(f"Epoch {epoch} OT: {ot} MMD: {mmd}")
                        print(f"Epoch {epoch} collision: {collision} FDE: {FDE}")
                        print(f"Epoch {epoch} dtw: {dtw} inter ped distance mmd: {ipd_mmd}")
                        
                        # self.log.info(f"Best of 20: Epoch {epoch} ADE: {ade} FDE: {fde}")
                        # self.log_writer.add_scalar('ADE', ade, epoch)
                        # self.log_writer.add_scalar('FDE', fde, epoch)
                        self.log.info(f"Epoch {epoch} MSE: {mse} MAE: {mae}")
                        self.log.info(f"Epoch {epoch} OT: {ot} MMD: {mmd}")
                        self.log.info(f"Epoch {epoch} collision: {collision} FDE: {FDE}")
                        self.log.info(f"Epoch {epoch} dtw: {dtw} inter ped distance mmd: {ipd_mmd}")
                        self.log.info(" ")
                        self.log_writer.add_scalar('MSE', mse, epoch)
                        self.log_writer.add_scalar('MAE', mae, epoch)
                        self.log_writer.add_scalar('OT', ot, epoch)
                        self.log_writer.add_scalar('MMD', mmd, epoch)
                        self.log_writer.add_scalar('Collision', collision, epoch)
                        self.log_writer.add_scalar('fde', FDE, epoch)
                        self.log_writer.add_scalar('dtw', dtw, epoch)
                        self.log_writer.add_scalar('ipd_mmd', ipd_mmd, epoch)
                        if self.config.save_model:
                            save_dir = os.path.join(self.logs_dir,'chpt')
                            print(f'save at epoch {epoch} to {save_dir}...')
                            os.makedirs(save_dir, exist_ok=True)
                            save_name = f'Epoch {epoch}' +'.pkl'
                            save_path = os.path.join(save_dir, save_name)
                            net_state_dict = self.model.state_dict()
                            torch.save(net_state_dict, save_path)
                        del traj_pred
                        # import gc
                        # gc.collect()
                        # torch.cuda.empty_cache()
                    elif self.config.val_mode=='singlestep':
                        val_mses=[]
                        for i, val_data in enumerate(val_list):
                            gt = val_data.labels[1:,:,4:6]
                            hist_fea = val_data.self_hist_features[:-1,:,:,:6]
                            ped_features = val_data.ped_features[:-1]
                            obs_features = val_data.obs_features[:-1]
                            self_feature = val_data.self_features[:-1,:,:]
                            context = (hist_fea, ped_features, self_feature, obs_features)
                            curr = val_data.labels[:-1,:,:6]
                            traj_pred = self.model.generate_onestep(context,curr)
                            mask = val_data.mask_a_pred[:-1]
                            # mask[0] = mask[1]
                            val_mse = mask_mse_func(traj_pred, gt, mask).item()
                            print(f"Val Dataset {i} Epoch {epoch} val mse {val_mse}")
                            self.log.info(f"Val Dataset {i} Epoch {epoch} val mse: {val_mse}")
                            self.log_writer.add_scalar(f'val_mse_{i}', val_mse, epoch)
                            val_mses.append(val_mse)
                    
                        val_mse_mean = np.mean(val_mses)
                        if self.config.save_model:
                            save_dir = os.path.join(self.logs_dir,'ckpt')
                            print(f'save at epoch {epoch} to {save_dir}...')
                            os.makedirs(save_dir, exist_ok=True)
                            save_name = f'epoch{epoch}_val_mse_'+format(val_mse_mean,'.5f')+'.pkl'
                            save_path = os.path.join(save_dir, save_name)
                            net_state_dict = self.model.state_dict()
                            torch.save(net_state_dict, save_path)
                
                self.model.train()


    def _build(self):
        self._build_dir()

        # self._build_encoder_config()
        # self._build_encoder()
        self._build_model()
        # self._build_train_loader()
        # self._build_eval_loader()
        if self.config.train_mode=='origin':
            self._build_train_loader2()
            self._build_eval_loader2()
        elif self.config.train_mode == 'multi':
            self._build_data_loader()
        else:
            raise NotImplementedError

        self._build_optimizer()
        

        #self._build_offline_scene_graph()
        #pdb.set_trace()
        print("> Everything built. Have fun :)")

    def _build_dir(self):
        self.model_dir = osp.join("./experiments",self.config.exp_name)
        import sys
        debug_flag = 'run' if sys.gettrace() ==None else 'debug'
        print('running in',debug_flag,'mode')
        logs_dir = osp.join(self.model_dir, time.strftime('%Y-%m-%d-%H-%M-%S'))
        logs_dir += debug_flag
        self.logs_dir = logs_dir
        os.makedirs(logs_dir,exist_ok=True)
        self.log_writer = SummaryWriter(log_dir=logs_dir)
        os.makedirs(self.model_dir,exist_ok=True)
        log_name = '{}.log'.format(time.strftime('%Y-%m-%d-%H-%M'))
        log_name = f"{self.config.dataset}_{log_name}"

        log_dir = osp.join(logs_dir, log_name)
        self.log = logging.getLogger()
        self.log.setLevel(logging.INFO)
        handler = logging.FileHandler(log_dir)
        handler.setLevel(logging.INFO)
        self.log.addHandler(handler)
        self.log.info(time.strftime('%Y-%m-%d-%H-%M-%S'))
        self.log.info("Config:")
        for item in self.config.items():
            self.log.info(item)

        self.log.info("\n")
        self.log.info("Eval on:")
        self.log.info(self.config.dataset)
        self.log.info("\n")

        print("> Directory built!")

    def _build_optimizer(self):
        self.optimizer = optim.Adam([
                                    # {'params': self.registrar.get_all_but_name_match('map_encoder').parameters()},
                                    {'params': self.model.parameters()}
                                    ],
                                    lr=self.config.lr,
                                    weight_decay=1e-5)
        self.scheduler = optim.lr_scheduler.StepLR(self.optimizer, step_size=10, gamma=0.999)
        if 'ft_lr' not in self.config.keys():
            if 'ucy' in self.config.data_dict_path:
                self.config.ft_lr=self.config.lr/100
            elif 'gc' in self.config.data_dict_path:
                self.config.ft_lr=self.config.lr/1000
            else:
                raise NotImplementedError
            # self.config.ft_lr=self.config.lr/1000
            # self.config.ft_lr=self.config.lr
        print('ft_lr:',self.config.ft_lr)
        self.ft_optimizer = optim.Adam([
                                    # {'params': self.registrar.get_all_but_name_match('map_encoder').parameters()},
                                    {'params': self.model.parameters()}
                                    ],
                                    lr=self.config.ft_lr,
                                    weight_decay=1e-5)
        if 'ucy' in self.config.data_dict_path:
            self.ft_scheduler = optim.lr_scheduler.StepLR(self.ft_optimizer, step_size=20, gamma=0.999)
        elif 'gc' in self.config.data_dict_path:
            # self.ft_scheduler = optim.lr_scheduler.StepLR(self.ft_optimizer, step_size=10, gamma=0.999)
            self.ft_scheduler = optim.lr_scheduler.StepLR(self.ft_optimizer, step_size=20, gamma=0.99)
        else:
            raise NotImplementedError
        self.log.info(f'(\'ft_lr\', {self.config.ft_lr})')
        print("> Optimizer built!")


    def _build_model(self):
        """ Define Model """
        config = self.config
        model = AutoEncoder(config, encoder = None)
        self.model = model.cuda()
        # if self.config.eval_mode:
        #     self.model.load_state_dict(self.checkpoint['ddpm'])
        if self.config.model_sd_path:
            self.model.load_state_dict(torch.load(self.config.model_sd_path))
        train_params = list(filter(lambda x: x.requires_grad, self.model.parameters()))
        trainable_params = np.sum([p.numel() for p in train_params])
        print('#Trainable Parameters:', trainable_params)
        self.log.info(f'#Trainable Parameters: {trainable_params}')
        self.log.info("\n")
        print("> Model built!")

   

    def _build_data_loader(self):

        if self.config.rebuild_dataset == True:
            if self.config.dataset_type=='timeindex':
                finetune_dataset = DATASET.TimeIndexedPedDataset2()    
            else:
                raise NotImplementedError
            finetune_dataset.load_data(self.config.data_config)
            
            print('number of finetune training dataset: ', len(finetune_dataset.raw_data['train']))
            finetune_dataset.build_dataset(self.config, finetune_flag=(self.config.finetune_trainmode=='multistep'))
            
            with open(self.config.data_dict_path, 'wb') as f:
                dill.dump(finetune_dataset, f, protocol=dill.HIGHEST_PROTOCOL)
        elif self.config.finetune == True:
            with open(self.config.data_dict_path, 'rb') as f:
                finetune_dataset = dill.load(f)


        
        if self.config.finetune == True:
            
            finetune_train_list = finetune_dataset.train_data
            if self.config.finetune_trainmode=='singlestep':
                self.finetune_train_loaders=[]
                assert type(finetune_train_list)==list
                for item in finetune_train_list:
                    self.finetune_train_loaders.append(DataLoader(
                                                item,
                                                batch_size=self.config.batch_size, 
                                                shuffle =False,
                                                drop_last=True))
            elif self.config.finetune_trainmode=='multistep':
                assert type(finetune_train_list[0])==DATA.ChanneledTimeIndexedPedData
                self.finetune_train_loaders = LOADER.data_loader(finetune_train_list, self.config.batch_size,
                                                self.config.seed, shuffle=False, drop_last=False)

            self.finetune_valid_list = finetune_dataset.valid_data

        return
        
        
    def _build_offline_scene_graph(self):
        if self.hyperparams['offline_scene_graph'] == 'yes':
            print(f"Offline calculating scene graphs")
            for i, scene in enumerate(self.train_scenes):
                scene.calculate_scene_graph(self.train_env.attention_radius,
                                            self.hyperparams['edge_addition_filter'],
                                            self.hyperparams['edge_removal_filter'])
                print(f"Created Scene Graph for Training Scene {i}")

            for i, scene in enumerate(self.eval_scenes):
                scene.calculate_scene_graph(self.eval_env.attention_radius,
                                            self.hyperparams['edge_addition_filter'],
                                            self.hyperparams['edge_removal_filter'])
                print(f"Created Scene Graph for Evaluation Scene {i}")
